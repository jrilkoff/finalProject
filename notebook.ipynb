{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.finbert as fb\n",
    "import src.ws_yahoo as wsy\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment CSV Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls website articles, puts into Dataframe\n",
    "# This takes quite a while to run\n",
    "\n",
    "def main_dataframe_creator(ticker_list, start, end):\n",
    "    folder_path = './csv/'\n",
    "    os.chdir(folder_path)   \n",
    "    \n",
    "    for item in ticker_list:\n",
    "        ticker = item\n",
    "        start = start\n",
    "        end = end\n",
    "        df = wsy.dataframe_price_sentiment(ticker, start, end)\n",
    "        df.to_csv(f'./{item}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses article CSVs to create sentiment csvs\n",
    "# This takes a VERY long time to run\n",
    "\n",
    "def sentiment_CSV():\n",
    "    csv_path = '../csv/'\n",
    "    sentiment_path = '../sentiment/'\n",
    "    extension = 'csv'\n",
    "    os.chdir(csv_path)\n",
    "    result = glob.glob('*.{}'.format(extension))\n",
    "    print(result)\n",
    "\n",
    "    for item in result:\n",
    "        os.chdir(csv_path)\n",
    "        print(item)\n",
    "        df_csv = pd.read_csv(item)\n",
    "        df_sentiment = fb.sentiment_poster(df_csv)\n",
    "        os.chdir(sentiment_path)\n",
    "        df_sentiment.to_csv(f'./{df_sentiment.company[0]}-sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls CSVs out from ../sentiment and puts into one single dataframe\n",
    "# This is the final dataset for feature engineering and the pipeline\n",
    "\n",
    "def sentiment_dataframe():\n",
    "    path = '../sentiment/'\n",
    "    extension = 'csv'\n",
    "    os.chdir(path)\n",
    "    result = glob.glob('*.{}'.format(extension))\n",
    "    print(result)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for item in result:\n",
    "        df = pd.read_csv(item, index_col=0)\n",
    "        df_list.append(df)\n",
    "\n",
    "    df_sentiment = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return df_sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticker News Article Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticker_list = ['WPM', 'PAAS', 'HL', 'MAG', 'CDE'] <- example\n",
    "# currently the ws_yahoo.py only grabs 2 articles, needs to be manually updated in script\n",
    "ticker_list = ['HBM', 'PAAS']\n",
    "start = '2017-01-01'\n",
    "end = '2022-11-30'\n",
    "\n",
    "main_dataframe_creator(ticker_list, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates all the sentiment CSVs from the news articles\n",
    "# This can take a VERY long time - about 30 seconds per article\n",
    "# Web scraper generally grabs 80-90 articles per ticker (if no preset limits on)\n",
    "\n",
    "sentiment_CSV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function compiles all the sentiment csvs into a single dataframe\n",
    "df = sentiment_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_set1(df):\n",
    "    # weight of sentiment, depending on which sentiment, the other two weighted against the dominant\n",
    "    df.loc[df['fb_body_stmt'] == 0, 'fb_body_weight'] = (df['fb_body_posi'] / (df['fb_body_neut'] + df['fb_body_nega']))\n",
    "    df.loc[df['fb_body_stmt'] == 1, 'fb_body_weight'] = (df['fb_body_nega'] / (df['fb_body_posi'] + df['fb_body_neut']))\n",
    "    df.loc[df['fb_body_stmt'] == 2, 'fb_body_weight'] = (df['fb_body_neut'] / (df['fb_body_posi'] + df['fb_body_nega']))\n",
    "    df.loc[df['fb_head_stmt'] == 0, 'fb_head_weight'] = (df['fb_head_posi'] / (df['fb_head_neut'] + df['fb_head_nega']))\n",
    "    df.loc[df['fb_head_stmt'] == 1, 'fb_head_weight'] = (df['fb_head_nega'] / (df['fb_head_posi'] + df['fb_head_neut']))\n",
    "    df.loc[df['fb_head_stmt'] == 2, 'fb_head_weight'] = (df['fb_head_neut'] / (df['fb_head_posi'] + df['fb_head_nega']))\n",
    "\n",
    "    # do the headlines and body sentiments align\n",
    "    df.loc[df['b_body_stmt'] == df['b_head_stmt'], 'b_alignment'] = 1\n",
    "    df.loc[df['b_body_stmt'] != df['b_head_stmt'], 'b_alignment'] = 0\n",
    "    df.loc[df['fb_body_stmt'] == df['fb_head_stmt'], 'fb_alignment'] = 1\n",
    "    df.loc[df['fb_body_stmt'] != df['fb_head_stmt'], 'fb_alignment'] = 0\n",
    "    df.loc[df['b_alignment'] == df['fb_alignment'], 'all_alignment'] = 1\n",
    "    df.loc[df['b_alignment'] != df['fb_alignment'], 'all_alignment'] = 0\n",
    "\n",
    "    # get dummies\n",
    "    df = pd.concat([df, pd.get_dummies(df['b_body_stmt'], prefix='b_body_stmt')], axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['b_head_stmt'], prefix='b_head_stmt')], axis=1)\n",
    "\n",
    "    df = pd.concat([df, pd.get_dummies(df['fb_body_stmt'], prefix='fb_body_stmt')], axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['fb_head_stmt'], prefix='fb_head_stmt')], axis=1)\n",
    "\n",
    "    columns_to_drop = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',  # Financial columns\n",
    "    'company', 'level_0', 'index', 'url', 'headline' , 'source', 'body', 'date' # Categorical columns\n",
    "    ]\n",
    "\n",
    "    df_final = df.drop(columns=columns_to_drop).reset_index(drop=True)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = feature_set1(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "## WARNINGS ARE TURNED OFF ##\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()  \n",
    "xboost = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datamodel Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datamodel_creation(df):\n",
    "    # Identify numeric and categorical features, columns to keep and drop\n",
    "    drop_feats = ['b_body_stmt', 'b_head_stmt', 'fb_body_stmt', 'fb_head_stmt']\n",
    "    target_feat = ['target']\n",
    "\n",
    "    df_target = df[target_feat]\n",
    "    df_pipeline = df.drop(columns=target_feat)\n",
    "    model_feats = df_pipeline.columns.tolist()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_pipeline, df_target, test_size=0.20, random_state=42)\n",
    "\n",
    "    datamodel = [X_train, y_train, X_test, y_test]\n",
    "\n",
    "    return datamodel, model_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    metrics_list = [recall, precision, accuracy, f1, roc_auc]\n",
    "    metrics_tag = ['recall', 'precision', 'accuracy', 'f1 score', 'roc auc score']\n",
    "    metrics_def = [\n",
    "        '--> Recall is the fraction correctly identified as positive out of all predicted positives',\n",
    "        '--> Precision is the fraction correctly identified as positive out of all positives',\n",
    "        '--> Accuracy is the fraction of predictions our model got correct',\n",
    "        '--> F1 Score is the harmonic mean of models precision and recall',\n",
    "        '--> ROC-AUC Score shows the performance of the model at all classification levels']\n",
    "\n",
    "    for i in range(len(metrics_list)):\n",
    "        print(f'the {metrics_tag[i]} is: {metrics_list[i]} {metrics_def[i]}')\n",
    "\n",
    "    print(f'the confusion matrix is:\\n{cm}')\n",
    "\n",
    "    return metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(model, datamodel):\n",
    "\n",
    "    numeric_transform = Pipeline([\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('select_k_best', SelectKBest(k=3))\n",
    "    ])\n",
    "\n",
    "    preprocessing = ColumnTransformer([\n",
    "        ('numeric', numeric_transform, model_feats),\n",
    "    ])\n",
    "\n",
    "    union = FeatureUnion([\n",
    "        ('pca', PCA(n_components=3)),\n",
    "        ('select_k_best', SelectKBest(k=3))\n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('features', union),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    if model == lr:\n",
    "        params = {\n",
    "            'features__pca__n_components': [1, 2, 3, 4, 5],\n",
    "            'features__select_k_best__k': [1, 2, 3, 4, 5],\n",
    "            'model__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            'model__penalty': ['1l', 'l2', 'elasticnet', 'none']\n",
    "        }\n",
    "    elif model == rfc:\n",
    "        params = {\n",
    "            'features__pca__n_components': [1, 2, 3, 4, 5],\n",
    "            'features__select_k_best__k': [1, 2, 3, 4, 5],\n",
    "            # 'model__criterion' : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "            'model__max_depth': [10, 25, 50, 100],\n",
    "            'model__n_jobs' : [-1],\n",
    "            # 'model__max_features': [\"sqrt\", \"log2\", None],\n",
    "            'model__n_estimators': [10, 50, 75, 100, 250]\n",
    "        }\n",
    "    elif model == gbc:\n",
    "        params = {\n",
    "            'features__pca__n_components': [1, 2, 3, 4, 5],\n",
    "            'features__select_k_best__k': [1, 2, 3, 4, 5],\n",
    "            'model__n_estimators': [50, 75, 100, 250, 500]\n",
    "        }\n",
    "    elif model == xboost:\n",
    "        params = {\n",
    "        'model__max_depth': [2, 3, 4, 5, 6],\n",
    "        'model__eta': [1, 2, 3], \n",
    "        'model__nthread': [-1],\n",
    "        'model__objective': ['binary:logistic']\n",
    "        }\n",
    "\n",
    "    # model = pipeline.fit(datamodel[0], datamodel[1])\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, params, verbose=0)\n",
    "    model = grid_search.fit(datamodel[0], datamodel[1])\n",
    "    print(f'The parameters were:\\n{grid_search.best_params_}') \n",
    "    print(f'The best score was: {grid_search.best_score_}')\n",
    "\n",
    "    y_pred = model.predict(datamodel[2])\n",
    "    y_prob = model.predict_proba(datamodel[2])\n",
    "    y_best = model.best_score_\n",
    "    # y_feats = grid_search.feature_importances_\n",
    "\n",
    "    metrics_list = metrics(datamodel[3], y_pred)\n",
    "    \n",
    "    return model, metrics_list, y_prob, y_pred, y_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodel, model_feats = datamodel_creation(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16.8s\n",
    "model_lr, metrics_lr, prob_lr, pred_lr, best_lr = pipeline(lr, datamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1m 25.6s\n",
    "model_rfc, metrics_rfc, prob_rfc, pred_rfc, best_rfc = pipeline(rfc, datamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21.7s\n",
    "model_gbc, metrics_gbc, prob_gbc, pred_gbc, best_gbc = pipeline(gbc, datamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8s\n",
    "model_xgb, metrics_xgb, prob_xgb, pred_xgb, best_xgb = pipeline(xboost, datamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Metrics and Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "metrics_tag = ['recall', 'precision', 'accuracy', 'f1 score', 'roc auc score']\n",
    "model_bests = [best_lr, best_rfc, best_gbc, best_xgb]\n",
    "\n",
    "df_met_lr = pd.DataFrame(np.array(metrics_lr).reshape(1,-1), columns=metrics_tag).T\n",
    "df_met_lr['model'] = 'Logistic Regression'\n",
    "df_met_rfc = pd.DataFrame(np.array(metrics_rfc).reshape(1,-1), columns=metrics_tag).T\n",
    "df_met_rfc['model'] = 'Random Forest'\n",
    "df_met_gbc = pd.DataFrame(np.array(metrics_gbc).reshape(1,-1), columns=metrics_tag).T\n",
    "df_met_gbc['model'] = 'Gradient Boosting'\n",
    "df_met_xgb = pd.DataFrame(np.array(metrics_xgb).reshape(1,-1), columns=metrics_tag).T\n",
    "df_met_xgb['model'] = 'XGBoost'\n",
    "\n",
    "\n",
    "df_met = pd.concat([df_met_lr, df_met_rfc, df_met_gbc, df_met_xgb])\n",
    "df_met = df_met.reset_index()\n",
    "df_met = df_met.rename(columns={'index':'metric', 0:'score'})\n",
    "df_met\n",
    "\n",
    "graph = pd.pivot_table(\n",
    "    df_met,\n",
    "    values=['score'],\n",
    "    index=['metric'],\n",
    "    columns=['model'],\n",
    "    aggfunc=np.sum,\n",
    ")\n",
    "\n",
    "graph.plot(\n",
    "    kind='bar', \n",
    "    ylim=(0,1),\n",
    "    title='Model Metrics',\n",
    "    figsize=(10,7),\n",
    "    fontsize=15,\n",
    "    rot=45,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
